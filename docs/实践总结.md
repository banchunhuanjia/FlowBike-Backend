
### 能跑 → 理解 → 微改 → 扩展 → 优化 → 重构

当你遇到问题的时候：

❌ 不要一来就说：帮我写怎么做
❌ 不要只贴报错截图
❌ 不要把指望放在我“修代码”

正确方式：

✔ 告诉我你想实现什么
✔ 给出你尝试过的思路
✔ 提供相关代码片段
✔ 说明你观察到的现象 / 预期行为

这样一来：

我能看出你的思考过程

你能学到更多

这个项目才真正属于你


**1.函数返回引用法则**

  直接按值返回 std::string，让编译器去优化。安全永远是第一位的
不过我们还是给一个判断流程

- 问题 1：我要返回的东西，是不是函数的参数之一？
  - 是： ✅ 非常安全！这是返回引用的最常见、最安全的用法。
  - 否： 🔴 危险！请继续问下一个问题
- 问题 2：我要返回的东西，是不是一个类的成员变量？
  - 是： 🟡 可能是安全的，但有风险。请继续问最关键的第 3 个问题。
  - 否： ❌ 绝对禁止！ 如果一个东西既不是函数参数，也不是类的成员变量，那它必然是一个局部变量。返回局部变量的引用是 C++ 的**“重罪”**，会导致程序立即崩溃或产生未定义行为。
- 问题 3 (最关键！)：拿到这个返回引用的代码，能不能比拥有这个数据的对象活得更久？
这个问题专门针对“返回类成员变量的引用”的情况。
  - 能： ❌ 绝对禁止！ 这就是“悬垂引用”的根源。
  - 不能 (即，对象保证比引用活得长)： ✅ 相对安全。这意味着对象的生命周期完全由你掌控，并且你知道它在整个引用使用期间都是有效的。

---
**2.optional的使用**
```c++
auto desc_opt = ErrorManager::getInstance().getDesc(999);
if (desc_opt.has_value()) {
    // 找到了
    std::cout << "Error description: " << desc_opt.value() << std::endl;
} else {
    // 没找到
    std::cout << "Error code 999 not found!" << std::endl;
}
```

---
**3.向前声明的使用法则**

在 .h 文件中，如果你只需要使用一个类的指针 (*) 或引用 (&)，就优先使用前向声明，而不是 #include。

---
**4.让抽象类更纯粹**

永远不能创建实例
```c++
protected:
  iEventHandler() = default;
```

---
**5.类型转换法则**

子转父static_cast
父转子dynamic_cast

---
**6.gdb+coredump思维流程**
```bash
0  0x00007f314a772078 in ___pthread_mutex_lock (
    mutex=0x7f314a77e66b <_int_malloc+3259>)
    at ./nptl/pthread_mutex_lock.c:136
1  0x000055bad5ae2963 in __gthread_mutex_lock (
    __mutex=0x7f314a77e66b <_int_malloc+3259>)
    at /usr/include/x86_64-linux-gnu/c++/11/bits/gthr-default.h:749
2  0x000055bad5ae2bea in std::mutex::lock (
    this=0x7f314a77e66b <_int_malloc+3259>)
    at /usr/include/c++/11/bits/std_mutex.h:100
3  0x000055bad5ae39ca in std::lock_guard<std::mutex>::lock_guard (
    this=0x7ffe86bb1830, __m=...)
    at /usr/include/c++/11/bits/std_mutex.h:229
4  0x000055bad5adf8d4 in DispatchMsgService::unsubscribe (
    this=0x7f314a77e62b <_int_malloc+3195>, eid=1)

    at /home/linyuan/FlowBike/src/dispatchmsgservice.cpp:26
5  0x000055bad5aebabb in UserEventHandler::~UserEventHandler (this=0x7ffe86bb1950, __in_chrg=<optimized out>) at /home/linyuan/FlowBike/src/usereventhandler.cpp:22
6  0x000055bad5ad9277 in main () at /home/linyuan/FlowBike/src/main.cpp:38
```
对照各种文件来看
- 0帧：错误在pthread_mutex_lock -> 与mutex状态有关
- 1，2，3帧定位是lock_guard使用锁
- 4帧告诉我们是退订阅的时候出错，所以这个锁是dms的，所以是dms实例出了问题
- 5帧告诉我们是usereventhandler的析构中调用退订阅出问题，所以是ueh的dms_出了问题
- 无效原因：
  - 曾有效，但是后面坏了 ->根据主函数执行结果发现析构顺序没问题
  - 一开始就无效 ->检查构造函数，发现真的是忘构造了

---
**7.字节序转换函数家族**

[h | n] to [s | l]
h: host n:network
s: short 常用于端口号port
l: long 常用于ipv4地址，传输协议的包长和类型ID

---
**8.程序逻辑**

因为在start函数初始化listener_时我们传入了第一棒的身份指针是NetworkInterface的this指针，而在TcpConnection
listener是工厂，bufferevent的产品上下文就是tcp
```bash
[main]
  |
  v
[new NetworkInterface(dms)] -> this_net_if
  |
  v
[start(8080)]
  |
  v
evconnlistener_new_bind(..., static_accept_cb, this_net_if)
  |
  +-------------------------------------------------------------+
  | (libevent is now holding this_net_if for the listener)      |
  +-------------------------------------------------------------+
  |
  | [A new client connects...]
  |
  v
[libevent calls static_accept_cb(..., void* ctx)]  <-- ctx IS this_net_if
  |
  v
[self = static_cast<NetworkInterface*>(ctx)]
  |
  v
[self->accept_new_connection(...)]
  |
  v
[new TcpConnection(..., self, ...)] -> new_conn
  |
  +------> [Inside TcpConnection constructor]
           |
           v
           bufferevent_setcb(bev, ..., static_read_cb, new_conn)
           |
           +------------------------------------------------------+
           | (libevent is now holding new_conn for this specific  |
           |  bufferevent)                                        |
           +------------------------------------------------------+
           |
           | [Client sends data...]
           |
           v
           [libevent calls static_read_cb(..., void* ctx)] <-- ctx IS new_conn
```
---
**9.客户端收发**  

send / write: 发送数据，把内存里的东西塞进内核缓冲区。
read: 接收数据，把内核缓冲区里的东西读到你的内存里。

---
**10.bufferevent**  

它其实是一个有缓存区的管家，解决复杂I/O管理，是个带缓冲的事件
  - 对于写只需要调用bufferevent_write()就不用管了，对于读只要设置编写read_cb即可
  - 无需自己去poll/select一个socket的状态，只需要设置回调函数

---
**11.闭环模型**
```bash
           【请求】
Client ---→ NetworkInterface ---→ DispatchMsgService ---→ UserEventHandler
                                                                  |
                                                                  ↓
Client ←--- NetworkInterface ←--- DispatchMsgService ←--- 生成 ResponseEvent
           【回包】
```
NetworkInterface 只收包回包不处理业务
DispatchMsgService 派发event到业务，然后收集ResponseEvent交回NetworkInterface
UserEventHandler 处理业务，返回ResponseEvent，不做网络操作

---
**12.服务端声明周期**
```bash
┌───────────────┐        ① TCP传输(PB序列化的二进制包)
│   客户端      │ ---------------------------------------------▶
└───────────────┘

              NetworkInterface（网络层 - IO线程/主线程）
              ─────────────────────────
                    ↓ ② libevent 回调收到数据 (bev, read_cb)
            ┌─────────────────────────────────────────┐
            │ 解析 header + body                       │
            │ new MobileCodeReqEv(...)                │ ◀─── ③ create_event
            │ ev->set_fd(bufferevent_getfd);   │      (★ 保存连接ID，而非指针)
            └─────────────────────────────────────────┘
                               │
                               │④ dms_->enqueue(std::move(ev))
                               ▼
              DispatchMsgService（调度层 - 线程池）
              ─────────────────────────
                               │⑤ 跨线程调度 process(ev)
                               ▼

              UserEventHandler（业务层 - 业务线程）
              ─────────────────────────
     ┌──────────────────────────────────────────────────┐
     │ 根据 dispatcher_ 查找处理函数                    │
     │ auto rsp = handle_mobile_code_req(req);          │
     │ rsp->set_fd(req->get_fd());               │─── ⑥ ★ FD接力：将ID传给响应
     │ delete req;                                      │─── ⑦ 释放请求事件内存
     └──────────────────────────────────────────────────┘
                               │
                               │⑧ post_response(std::move(rsp))
                               ▼

              DispatchMsgService（调度层）
              ─────────────────────────
                   network_interface->send_response(rsp)
                               │⑨ 放入发送队列 (线程安全)
                               ▼

              NetworkInterface（网络层 - IO线程/主线程）
              ─────────────────────────
     ┌──────────────────────────────────────────────────┐
     │ std::lock_guard lock(resp_mtx);           │
     │ resp_queue.push(std::move(packet));              │─── ⑩ 仅入队，不立即发送
     │ (Timer触发 flush_send_queue)                     │─── ⑪ 异步发送
     │ 查找 connections_.find(fd)                       │─── ★ 检查连接是否还活着
     │ bufferevent_write(bev, header+body)              │─── ⑫ 安全回发
     └──────────────────────────────────────────────────┘

┌───────────────┐        ⑬ TCP传输(PB序列化的二进制包)
│   客户端      │ ◀---------------------------------------------
└───────────────┘
```
📌 面试时的讲解重点：

步骤 ③ (set_fd)：强调这是解决 Segfault 的关键。我们切断了业务层对底层网络对象 (bufferevent) 的直接依赖，只传递一个轻量级的 ID。  

步骤 ⑥ (FD 接力)：强调业务逻辑的闭环。业务层负责告诉底层“这个响应是回复给哪个连接的”。  

步骤 ⑩ ~ ⑪ (异步队列)：强调One Loop One Thread原则。我们禁止业务线程直接操作 Socket，而是把数据扔回给主线程（IO线程）去发。这不仅解决了竞争问题，还保证了发送顺序。  

步骤 ⑪ (find fd)：强调容错性。如果客户端在业务处理期间断开了，主线程查不到 fd 就会安全丢弃，而不会因为访问野指针而崩溃。  

---
**13.delete相关**  

对于静态类型，其产生的对象是存在栈内的，并非堆
而new出来的是堆内存，他需要释放。而栈内的内存会被自动清理

---
**14.线程的join和detach**  

join是加入汇合，线程会阻塞等待
detach是分离，非阻塞，跑各的

---
**15.connectionpool内的设计**  

produceConnection的push并未上锁，是因为getConnection需要锁，而再加一层会死锁
如果不加，运行时（getConnection）：靠外层的锁来保证安全（防止死锁）启动时（init）：靠“单线程环境”来保证安全（因为那时候还没人跟它抢），不会出现线程安全问题
这被称为无“锁 helper 函数 + 有锁 public 接口”模式


---
**16.一些回答答案**  

1.“封装是为了遵循 RAII 原则，利用 C++ 对象的生命周期来管理 C 语言的原始资源，防止忘记调用 mysql_close 导致资源泄漏。

关于 freeResult，不仅仅是为了代码复用，更重要的是为了防止内存泄漏。因为 mysql_store_result 会在堆上分配内存，如果我在复用连接进行下一次查询前不释放上一次的结果，旧的内存地址就会丢失，导致严重的内存泄漏。将其独立封装能保证在‘析构时’和‘新查询前’都能安全地清理资源。”

2.“关于自动回收，我确实使用了 shared_ptr 的自定义删除器。当业务层引用计数归零时，Lambda 表达式会拦截删除操作，将连接重新 push 回队列，实现复用。

关于线程安全：因为连接池是多线程共享的，我在操作队列（取连接/还连接）时使用了 std::mutex（互斥锁） 来保证原子性。同时，我使用了 std::condition_variable（条件变量） 来实现生产-消费模型：当队列为空时，消费者会阻塞等待；当有连接归还时，会自动唤醒等待的线程，避免了轮询造成的 CPU 浪费。”

3.“DAO 确实是为了解耦。

关于为什么用 INSERT ... ON DUPLICATE KEY UPDATE，除了代码简洁外，主要是为了解决并发场景下的竞态条件 (Race Condition)。

如果我用‘先查询(Select)后插入(Insert)’的两步逻辑： 在高并发下，可能有两个线程 A 和 B 同时查到用户不存在。然后 A 插入成功了，B 再去插入时就会报**‘主键冲突’**错误。 使用这条原子性的 SQL 语句，将检查和插入合并由数据库底层加锁处理，彻底避免了这种并发冲突，保证了操作的原子性。”

4.场景： 你在 UserEventHandler 里调用了 auto conn = pool->getConnection();。 假设在这个函数执行过程中，抛出了一个异常（比如 std::stoi 转换失败），导致函数提前退出，没有执行到最后的 return。 请问： 这个连接会泄漏吗？为什么？这体现了 C++ 的什么优势？

流程：抛出异常 -> 触发栈展开 -> conn 智能指针析构 -> 触发自定义删除器 Lambda -> 执行 push() -> 连接回到队列。
结论：这就是 C++ RAII 最强大的地方——只要对象创建成功，无论你怎么死（return 还是 exception），收尸工作（析构）一定会执行。 所以你的连接池是“异常安全”的

---
**17.快捷查找**
```bash
grep -rn "xx" .
```
在当前目录递归查找并显示行数的搜索xx

---
**18.我一点头绪都没有的问题**  

1.你提到用了 CAS (Compare And Swap) 来解决并发开锁。 如果线程 A 查到状态是 0。 这时候线程 B 把状态改成 1（开锁），然后又迅速改回 0（还车）。 轮到线程 A 执行 Update 时，它发现状态还是 0，于是开锁成功了。 在这个共享单车的场景下，这种 ABA 问题会有副作用吗？为什么？
答：
  这个场景ABA并没有副作用，AB互相没有影响，但是在状态变化需要记录的场景ABA问题就会出错（比如首单免单的ABA），如果想解决的话就是加入版本号，每次update计入版本号的更改，版本号变了更新就失效

---
**19.一次更新的理由**  

通过模拟的问题，我发现我们的业务逻辑没法解决“如果扣了用户的钱（假设你有扣费逻辑），但是开锁失败了”的问题

为了解决这个问题，我们决定对函数进行重载。因为这是两个表，彼此间没有连接，自然无法执行跨表任务，那解决方案自然就是建立连接，只需要我们重载两个DAO函数的接口，加入一个开启事务的连接的参数并利用，最后由业务层统一conn->commit()即可
也就是从连接池申请唯一的一个数据库连接 (conn)。

一声令下：conn->transaction() 开启事务（此时数据库进入“录像模式”）。
协同作战：
指挥 UserDao 查人（必须用刚才那个 conn）。
指挥 BikeDao 查车（必须用刚才那个 conn）。
指挥 BikeDao 改状态（必须用刚才那个 conn）。

对于代码中的参数
如果传进来的是空，就去池里拿；如果不是空，就用传进来的。
注意：如果去池里拿，需要用 shared_ptr 自动回收；如果是传进来的，我们不负责回收（业务层负责）

---
**20.关于拷贝**  

MysqlConn* a = b;是浅拷贝，只是一个a只是句柄，借用b，a不是一个新对象
只有非指针对象的=才存在拷贝构造的问题

---
**21.mysqlAPI返回值**  

MySQL 的 C API 函数（如 mysql_autocommit, mysql_commit, mysql_rollback）遵循的是 C 语言的老规矩：
返回 0：表示成功 (Success)。
返回 非0：表示失败 (Error)。

这和我们平时的返回逻辑不太一样，所以要注意，我们第一次用所以错了一次

---
**22.redis与mysql策略及原因**  

1. 为什么 Redis 可以用 Thread Local？
资源轻量级：Redis 是基于内存的，连接处理非常快。一个 Redis 实例维持几千个空闲连接毫不费力。

操作原子且极快：Redis 的命令通常是微秒级（us）。

线程数固定：你的 ThreadPool 只有 4-8 个线程。这意味着整个进程只会有 4-8 个 Redis 连接。这点开销对 Redis 来说是九牛一毛。

无事务上下文：在你的用法里，Redis 只是存取 Token，不需要像 MySQL 那样开启一个长事务占用连接很久。

结论：给每个线程发一个“私人 Redis 连接”是划算的，是用极小的空间成本换取了无锁的高性能。

2. 为什么 MySQL 必须用连接池 (Singleton Pool)？
A. 连接极其昂贵 (Heavyweight)
MySQL 服务端：每一个 TCP 连接，MySQL 服务端都要分配线程栈、连接缓冲、排序缓冲等内存。

连接数限制：MySQL 默认的 max_connections 通常只有 151（生产环境可能调到 1000-2000）。

如果用 Thread Local：

现在你只有 4 个线程，没问题（4 个连接）。

但如果以后为了提高并发，你把线程池开到了 500 个线程（为了掩盖 I/O 延迟）。

那么你就需要建立 500 个 MySQL 连接。MySQL 服务端可能直接报警甚至挂掉。

B. 复用率与削峰填谷 (Multiplexing)
场景：线程 A 正在处理一个不需要查数据库的请求（比如只查 Redis），线程 B 正在处理一个复杂的 SQL。

Thread Local：线程 A 的 MySQL 连接在睡大觉（闲置浪费），而线程 B 可能因为没有连接可用而干着急（如果没限制的话）。

连接池：连接是公共资源。谁要用谁拿，用完赶紧还回来给别人用。

核心价值：用少量的连接（比如 10 个）支撑大量的并发线程（比如 100 个）。

C. 锁的开销 vs I/O 开销
Redis：操作耗时 0.1ms。如果加锁（耗时 0.01ms），锁的开销占了 10%，很肉疼。所以我们要去锁。

MySQL：操作耗时 10ms ~ 500ms。如果加锁取连接（耗时 0.01ms），几乎可以忽略不计。

为了省这点锁的时间，放弃连接池的保护机制，是得不偿失的。

---
**23.在当前线程使用redis_tool**  

使用static thread_local
只有验证码对比成功我们才给token

---
**24.我是傻逼** 

💀 陷阱一：String 的“空指针死刑”
报错特征： terminate called after throwing an instance of 'std::logic_error' what(): basic_string::_M_construct null not valid

核心原理： C++ 的 std::string 不像 Java 或 Go 的 String 那样宽容。绝对不能用 NULL (0, nullptr) 去构造或赋值给 std::string，否则直接崩溃。

初始化列表：什么都不写（默认就是空串），或者写 ""。
```C++
MyClass() : ip_() {} // 或者 ip_("")
```
C 接口封装：必须防御性判空。
```C++
const char* val = c_library_get_value("key");
string s = (val != nullptr) ? val : ""; // 安全
```


⛓️ 陷阱二：Protobuf 修改的“全链路遗忘”
现象描述： .proto 文件改了，make 也编译过了，客户端也没问题，但服务端业务层死活读不到新加的字段（比如 Token 为空）。

核心原理： 我们的架构是 分层解耦 的。Protobuf 只是数据载体，数据从网卡流向业务逻辑，中间经过了多次**“搬运”**。改了源头（Proto），如果不改中间的搬运工，数据就会在中间丢掉。

🛠️ 协议修改标准检查清单 (Checklist)：

当你给 .proto 增加了一个字段（例如 token）时，必须按顺序检查以下 4 个地方：

- 源头：Proto 定义
  修改 xxx.proto，执行 make 重新生成 pb.h/cc。

- 搬运工 A：事件包装类定义 (.h) 
  修改 UnlockReq 类，构造函数增加参数 const string& token。
  增加 getter 方法 getToken()。

- 搬运工 B：事件包装类实现 (.cpp)
  最容易忘！ 修改构造函数的实现，把参数存进 msg_.set_token(token)。如果不加这行，参数传进来了也没用。

- 搬运工 C：工厂生产线 (DispatchMsgService)
  最容易忘！ 在 create_event 的 case 里，从 proto 对象取出 token()，传给 UnlockReq 的新构造函数。

口诀：
“改了 Proto 别急跑，工厂、构造、Getter 一个不能少。”

---
**25.redis使用连接池会喧宾夺主的真相** 

核心关键词：锁竞争 (Lock Contention)。

Redis 的操作极快（微秒级）。如果用连接池，所有线程都要去抢那一把 mutex 锁。在高并发下，线程在锁上排队的时间，可能比 Redis 执行命令的时间还长。这才是“喧宾夺主”的真相。

---
**26.不使用异步日志的原因**  

虽然确实能解决写磁盘阻塞业务线程，但是经过分析我发现我的平静主要在于日志量过大，会刷屏，即使用了异步日志，也会占用大量cpu去做字符串格式化和内存拷贝
对于这个问题我选择了从源头治理，使用日志分级，直接把无关日志过滤掉，不让它们产生任何 CPU 和 I/O 消耗。这比异步写垃圾日志更高效。